Organics
	Описание датасета
	В датасете нашлись пропущенные значения (7664), строим таблицу, чтобы посмотреть 
	количество и процент пропущенных значений для каждой переменной:
		больше всего пропущенно значений Gender - 2512 - 11.3%
	Изменение пропущенных значений:
		Удаление Nan df.dropna()
		Замена Nan усредненными значениями df = df.fillna(df.mean())
		Замена Nan наиболее часто встречаемыми значениями df = df.fillna(df.mod)
	
		После замены усредненными значениями из таблицы пропущенных значений 
		пропали переменные DemAge, DemAffl, DemCluster, PromTime - все переменные
		типа float64
		
		После изменения пропущенных на моду пропали значения номинальных переменных-
		DemGender, DemClusterGroup, DemReg, DemTVReg	
		
		
	Обучение модели:
		за Х берем все данные, кроме таргета
		за y - таргет	
		Разделяем выборку на тестовую и трейновую
		С помощью pydot и export_graphviz визуализируем дерево
		Находим оценку для 2 выборок

а) Обучим дерево с параметрами criterion = entropy и max_depth =5
	оценка на train - 0,806
		на test - 0,804
C помощью PrettyTable построим таблицу, которая состоит из разных данных, необходимых для построения
дерева. Рассмотрим все деревья, построенные на основе этих данных и найдем зависимостью

C увеличением глубины с определенного момента оценка для тестовой выборки начнет уменьшаться, а для тренировочной - увеличиваться. 
Оптимальная глубина - 5-6.

Titanic
Дерево (criterion='gini', max_depth=4,
                       max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2)
Оценка на тестовой выборке больше, чем на трейновой 